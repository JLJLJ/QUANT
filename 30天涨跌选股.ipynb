{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import QUANTAXIS as QA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import  pprint as print\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def func_scale(df,groups):\n",
    "    #部分不同列的数据具有相关性，需要统一标准化\n",
    "    for group in groups:\n",
    "        if type(group)!=list and type(group)!=tuple:\n",
    "            group=[group]\n",
    "        assemble_scale_column=group\n",
    "        df[assemble_scale_column] = preprocessing.scale(np.ravel(df[assemble_scale_column])).reshape([len(df),-1])\n",
    "    return df\n",
    "def standardization(df,groups):\n",
    "    #对数据去中心化，使符合正态分布\n",
    "    df=df.groupby('code').apply(func_scale,groups)\n",
    "    return df\n",
    "\n",
    "def func_series_to_supervised(dataset, n_in=1):\n",
    "    #dataset = pd.DataFrame(data)\n",
    "    df_all=pd.DataFrame()\n",
    "    \n",
    "    for i in range(n_in, 0, -1):\n",
    "        df_back=dataset.shift(i)\n",
    "        col_names=df_back.columns+'_'+str(i)\n",
    "        df_back.columns=col_names\n",
    "        df_all=pd.concat([df_all,df_back],axis=1)\n",
    "        #print(df_all.index)\n",
    "    return df_all\n",
    "#series_to_supervised(dataset,3)\n",
    "def series_to_supervised(dataset, n_in=1):\n",
    "    return dataset.groupby('code').apply(func_series_to_supervised,n_in)\n",
    "\n",
    "def max_min_close_n_days(data, n=30):\n",
    "    '''\n",
    "    自定义指标，计算当日（不含）到n天后收盘价最大值、最小值、最大值上涨比率、最小值下跌比率\n",
    "    '''\n",
    "    n_days_max=pd.Series(np.zeros(len(data)))\n",
    "    n_days_min=pd.Series(np.zeros(len(data)))\n",
    "    n_days_max_radio=pd.Series(np.zeros(len(data)))\n",
    "    n_days_min_radio=pd.Series(np.zeros(len(data)))\n",
    "    if(len(data)-n<0):\n",
    "        '''for i in range(len(data)):\n",
    "            n_days_max[i]=None\n",
    "            n_days_min[i]=None'''\n",
    "        n_days_max[0:len(data)]=None\n",
    "        n_days_min[0:len(data)]=None\n",
    "    else:\n",
    "        for i in range(len(data)-n):\n",
    "            n_days_max[i]=(max(data['close'][i+1:i+n+1]))\n",
    "            n_days_min[i]=(min(data['close'][i+1:i+n+1]))\n",
    "            n_days_max_radio[i]=(n_days_max[i]-data['close'][i])/data['close'][i]\n",
    "            n_days_min_radio[i]=(n_days_min[i]-data['close'][i])/data['close'][i]\n",
    "        n_days_max[len(data)-n:len(data)]=None\n",
    "        n_days_min[len(data)-n:len(data)]=None\n",
    "        '''for i in range(len(data)-n,len(data)):\n",
    "            n_days_max[i]=None\n",
    "            n_days_min[i]=None\n",
    "            '''\n",
    "    max_min_close_n_days=pd.DataFrame({'n_days_max':n_days_max,'n_days_min':n_days_min,\n",
    "                                       'n_days_max_radio':n_days_max_radio,'n_days_min_radio':n_days_min_radio})\n",
    "    max_min_close_n_days.index=data.index\n",
    "    return max_min_close_n_days\n",
    "\n",
    "def get_all_indicator(data):\n",
    "    dataset=pd.DataFrame()\n",
    "\n",
    "    indicator=QA.QA_indicator_ADTM(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ARBR(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ASI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ATR(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_BBI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    #indicator=QA.QA_indicator_BIAS(data)\n",
    "    #dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_BOLL(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_CCI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_CHO(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_DDI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_DMA(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_DMI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    #indicator=QA.QA_indicator_EMA(data)\n",
    "    #dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_EXPMA(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_KDJ(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MA(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MA_VOL(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MACD(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MFI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MIKE(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MTM(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_OBV(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_OSC(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_PBX(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_PVT(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ROC(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_RSI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VPT(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VR(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VRSI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VSTD(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    #indicator=QA.QA_indicator_WR(data)\n",
    "    #dataset=pd.concat([dataset,indicator],axis=1)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=100\n",
    "n_days=1\n",
    "stockCode=QA.QA_fetch_stock_list()['code'].tolist()\n",
    "stockCode=stockCode[0:k]\n",
    "data = QA.QA_fetch_stock_day_adv(stockCode, '2019-01-01', '2019-09-30')\n",
    "\n",
    "#1、生成标签（y）\n",
    "#获取当前股票20天后的最大涨幅和最小涨幅\n",
    "Y = data.add_func(max_min_close_n_days,n_days)\n",
    "#dataset=pd.concat([y,data.data], axis=1).dropna(thresh=9)\n",
    "\n",
    "#2、生成样本（x）\n",
    "dataset1=get_all_indicator(data)\n",
    "\n",
    "#3、标准化样本数据\n",
    "dataset2=standardization(data.data,(['open','close','high','low'],'volume','amount'))\n",
    "\n",
    "#####4、合并样本\n",
    "dataset=pd.concat([dataset1,dataset2],axis=1)\n",
    "\n",
    "#####5、时间序列转监督学习（n天的数据生成一个样本）\n",
    "dataset=series_to_supervised(dataset,10)\n",
    "\n",
    "#####定义Y值，加入样本中\n",
    "Y_names=['n_days_max_radio','n_days_min_radio']\n",
    "dataset=pd.concat([dataset,Y[Y_names]],axis=1)\n",
    "\n",
    "#####删除空值超过90%的样本\n",
    "len_columns=len(dataset.columns)\n",
    "dataset=dataset.dropna(thresh=len_columns*0.9)\n",
    "\n",
    "#####生成X,Y\n",
    "dataset=dataset.astype('float16')\n",
    "Y=dataset[Y_names]\n",
    "X=dataset.drop(Y_names,axis=1)\n",
    "\n",
    "X_train,Y_train=X,Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stockCode=QA.QA_fetch_stock_list()['code'].tolist()\n",
    "stockCode=stockCode[0:k]\n",
    "data = QA.QA_fetch_stock_day_adv(stockCode, '2019-10-01', '2019-12-31')\n",
    "\n",
    "#1、生成标签（y）\n",
    "#获取当前股票20天后的最大涨幅和最小涨幅\n",
    "Y = data.add_func(max_min_close_n_days,n_days)\n",
    "#dataset=pd.concat([y,data.data], axis=1).dropna(thresh=9)\n",
    "\n",
    "#2、生成样本（x）\n",
    "dataset1=get_all_indicator(data)\n",
    "\n",
    "#3、标准化样本数据\n",
    "dataset2=standardization(data.data,(['open','close','high','low'],'volume','amount'))\n",
    "\n",
    "#####4、合并样本\n",
    "dataset=pd.concat([dataset1,dataset2],axis=1)\n",
    "\n",
    "#####5、时间序列转监督学习（n天的数据生成一个样本）\n",
    "dataset=series_to_supervised(dataset,10)\n",
    "\n",
    "#####定义Y值，加入样本中\n",
    "Y_names=['n_days_max_radio','n_days_min_radio']\n",
    "dataset=pd.concat([dataset,Y[Y_names]],axis=1)\n",
    "\n",
    "#####删除空值超过90%的样本\n",
    "len_columns=len(dataset.columns)\n",
    "dataset=dataset.dropna(thresh=len_columns*0.9)\n",
    "\n",
    "#####生成X,Y\n",
    "dataset=dataset.astype('float16')\n",
    "Y=dataset[Y_names]\n",
    "X=dataset.drop(Y_names,axis=1)\n",
    "\n",
    "X_test,Y_test=X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=7)\n",
    "Y_train=Y_train['n_days_max_radio']\n",
    "Y_test=Y_test['n_days_max_radio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l1: 0.0191759\tvalid_0's l1: 0.0132499\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[2]\ttraining's l1: 0.0191732\tvalid_0's l1: 0.0132494\n",
      "[3]\ttraining's l1: 0.0191703\tvalid_0's l1: 0.013249\n",
      "[4]\ttraining's l1: 0.0191675\tvalid_0's l1: 0.0132485\n",
      "[5]\ttraining's l1: 0.0191646\tvalid_0's l1: 0.013248\n",
      "[6]\ttraining's l1: 0.0191616\tvalid_0's l1: 0.0132473\n",
      "[7]\ttraining's l1: 0.0191586\tvalid_0's l1: 0.0132468\n",
      "[8]\ttraining's l1: 0.0191552\tvalid_0's l1: 0.0132467\n",
      "[9]\ttraining's l1: 0.019152\tvalid_0's l1: 0.0132465\n",
      "[10]\ttraining's l1: 0.0191488\tvalid_0's l1: 0.013246\n",
      "[11]\ttraining's l1: 0.0191458\tvalid_0's l1: 0.0132457\n",
      "[12]\ttraining's l1: 0.019143\tvalid_0's l1: 0.0132447\n",
      "[13]\ttraining's l1: 0.01914\tvalid_0's l1: 0.0132442\n",
      "[14]\ttraining's l1: 0.0191373\tvalid_0's l1: 0.0132437\n",
      "[15]\ttraining's l1: 0.0191345\tvalid_0's l1: 0.0132433\n",
      "[16]\ttraining's l1: 0.0191316\tvalid_0's l1: 0.0132437\n",
      "[17]\ttraining's l1: 0.0191291\tvalid_0's l1: 0.0132433\n",
      "[18]\ttraining's l1: 0.0191261\tvalid_0's l1: 0.0132428\n",
      "[19]\ttraining's l1: 0.019123\tvalid_0's l1: 0.0132429\n",
      "[20]\ttraining's l1: 0.0191206\tvalid_0's l1: 0.0132426\n",
      "[21]\ttraining's l1: 0.0191183\tvalid_0's l1: 0.0132414\n",
      "[22]\ttraining's l1: 0.0191155\tvalid_0's l1: 0.0132419\n",
      "[23]\ttraining's l1: 0.0191128\tvalid_0's l1: 0.013241\n",
      "[24]\ttraining's l1: 0.01911\tvalid_0's l1: 0.0132406\n",
      "[25]\ttraining's l1: 0.0191077\tvalid_0's l1: 0.0132415\n",
      "[26]\ttraining's l1: 0.0191054\tvalid_0's l1: 0.0132411\n",
      "[27]\ttraining's l1: 0.0191031\tvalid_0's l1: 0.0132411\n",
      "[28]\ttraining's l1: 0.0191008\tvalid_0's l1: 0.0132406\n",
      "[29]\ttraining's l1: 0.0190985\tvalid_0's l1: 0.0132401\n",
      "[30]\ttraining's l1: 0.0190961\tvalid_0's l1: 0.0132392\n",
      "[31]\ttraining's l1: 0.0190936\tvalid_0's l1: 0.0132384\n",
      "[32]\ttraining's l1: 0.0190909\tvalid_0's l1: 0.0132382\n",
      "[33]\ttraining's l1: 0.0190885\tvalid_0's l1: 0.0132373\n",
      "[34]\ttraining's l1: 0.0190857\tvalid_0's l1: 0.0132368\n",
      "[35]\ttraining's l1: 0.019083\tvalid_0's l1: 0.0132367\n",
      "[36]\ttraining's l1: 0.0190806\tvalid_0's l1: 0.0132368\n",
      "[37]\ttraining's l1: 0.0190785\tvalid_0's l1: 0.0132369\n",
      "[38]\ttraining's l1: 0.019076\tvalid_0's l1: 0.0132368\n",
      "[39]\ttraining's l1: 0.0190735\tvalid_0's l1: 0.0132361\n",
      "[40]\ttraining's l1: 0.0190712\tvalid_0's l1: 0.0132359\n",
      "[41]\ttraining's l1: 0.0190687\tvalid_0's l1: 0.0132358\n",
      "[42]\ttraining's l1: 0.0190659\tvalid_0's l1: 0.0132356\n",
      "[43]\ttraining's l1: 0.0190628\tvalid_0's l1: 0.0132352\n",
      "[44]\ttraining's l1: 0.0190606\tvalid_0's l1: 0.0132352\n",
      "[45]\ttraining's l1: 0.019058\tvalid_0's l1: 0.0132353\n",
      "[46]\ttraining's l1: 0.0190546\tvalid_0's l1: 0.0132347\n",
      "[47]\ttraining's l1: 0.0190512\tvalid_0's l1: 0.0132344\n",
      "[48]\ttraining's l1: 0.0190479\tvalid_0's l1: 0.0132341\n",
      "[49]\ttraining's l1: 0.0190445\tvalid_0's l1: 0.0132338\n",
      "[50]\ttraining's l1: 0.0190414\tvalid_0's l1: 0.0132333\n",
      "[51]\ttraining's l1: 0.0190395\tvalid_0's l1: 0.0132334\n",
      "[52]\ttraining's l1: 0.0190372\tvalid_0's l1: 0.013233\n",
      "[53]\ttraining's l1: 0.0190343\tvalid_0's l1: 0.0132321\n",
      "[54]\ttraining's l1: 0.0190317\tvalid_0's l1: 0.013232\n",
      "[55]\ttraining's l1: 0.0190296\tvalid_0's l1: 0.0132316\n",
      "[56]\ttraining's l1: 0.0190269\tvalid_0's l1: 0.0132311\n",
      "[57]\ttraining's l1: 0.0190241\tvalid_0's l1: 0.0132314\n",
      "[58]\ttraining's l1: 0.0190218\tvalid_0's l1: 0.0132312\n",
      "[59]\ttraining's l1: 0.0190186\tvalid_0's l1: 0.0132305\n",
      "[60]\ttraining's l1: 0.0190162\tvalid_0's l1: 0.0132304\n",
      "[61]\ttraining's l1: 0.0190139\tvalid_0's l1: 0.0132301\n",
      "[62]\ttraining's l1: 0.0190116\tvalid_0's l1: 0.01323\n",
      "[63]\ttraining's l1: 0.0190091\tvalid_0's l1: 0.0132291\n",
      "[64]\ttraining's l1: 0.0190067\tvalid_0's l1: 0.0132288\n",
      "[65]\ttraining's l1: 0.0190046\tvalid_0's l1: 0.0132282\n",
      "[66]\ttraining's l1: 0.0190018\tvalid_0's l1: 0.0132284\n",
      "[67]\ttraining's l1: 0.0189987\tvalid_0's l1: 0.0132282\n",
      "[68]\ttraining's l1: 0.018996\tvalid_0's l1: 0.0132277\n",
      "[69]\ttraining's l1: 0.0189935\tvalid_0's l1: 0.0132274\n",
      "[70]\ttraining's l1: 0.0189906\tvalid_0's l1: 0.0132277\n",
      "[71]\ttraining's l1: 0.0189883\tvalid_0's l1: 0.0132273\n",
      "[72]\ttraining's l1: 0.018986\tvalid_0's l1: 0.0132271\n",
      "[73]\ttraining's l1: 0.0189834\tvalid_0's l1: 0.0132265\n",
      "[74]\ttraining's l1: 0.0189808\tvalid_0's l1: 0.013226\n",
      "[75]\ttraining's l1: 0.0189781\tvalid_0's l1: 0.0132247\n",
      "[76]\ttraining's l1: 0.0189757\tvalid_0's l1: 0.0132243\n",
      "[77]\ttraining's l1: 0.0189732\tvalid_0's l1: 0.0132241\n",
      "[78]\ttraining's l1: 0.0189704\tvalid_0's l1: 0.013224\n",
      "[79]\ttraining's l1: 0.018968\tvalid_0's l1: 0.0132235\n",
      "[80]\ttraining's l1: 0.0189657\tvalid_0's l1: 0.0132232\n",
      "[81]\ttraining's l1: 0.0189632\tvalid_0's l1: 0.0132234\n",
      "[82]\ttraining's l1: 0.0189609\tvalid_0's l1: 0.0132232\n",
      "[83]\ttraining's l1: 0.0189586\tvalid_0's l1: 0.0132226\n",
      "[84]\ttraining's l1: 0.0189562\tvalid_0's l1: 0.0132224\n",
      "[85]\ttraining's l1: 0.0189539\tvalid_0's l1: 0.0132218\n",
      "[86]\ttraining's l1: 0.0189511\tvalid_0's l1: 0.0132216\n",
      "[87]\ttraining's l1: 0.018949\tvalid_0's l1: 0.0132213\n",
      "[88]\ttraining's l1: 0.0189465\tvalid_0's l1: 0.0132212\n",
      "[89]\ttraining's l1: 0.0189438\tvalid_0's l1: 0.0132212\n",
      "[90]\ttraining's l1: 0.0189409\tvalid_0's l1: 0.013221\n",
      "[91]\ttraining's l1: 0.0189383\tvalid_0's l1: 0.0132208\n",
      "[92]\ttraining's l1: 0.0189354\tvalid_0's l1: 0.0132202\n",
      "[93]\ttraining's l1: 0.0189328\tvalid_0's l1: 0.0132196\n",
      "[94]\ttraining's l1: 0.0189303\tvalid_0's l1: 0.0132194\n",
      "[95]\ttraining's l1: 0.0189276\tvalid_0's l1: 0.0132191\n",
      "[96]\ttraining's l1: 0.0189249\tvalid_0's l1: 0.0132184\n",
      "[97]\ttraining's l1: 0.0189223\tvalid_0's l1: 0.0132182\n",
      "[98]\ttraining's l1: 0.01892\tvalid_0's l1: 0.0132175\n",
      "[99]\ttraining's l1: 0.0189172\tvalid_0's l1: 0.0132171\n",
      "[100]\ttraining's l1: 0.0189148\tvalid_0's l1: 0.0132168\n",
      "[101]\ttraining's l1: 0.018912\tvalid_0's l1: 0.013217\n",
      "[102]\ttraining's l1: 0.0189092\tvalid_0's l1: 0.0132172\n",
      "[103]\ttraining's l1: 0.0189065\tvalid_0's l1: 0.0132176\n",
      "[104]\ttraining's l1: 0.0189038\tvalid_0's l1: 0.0132173\n",
      "[105]\ttraining's l1: 0.0189011\tvalid_0's l1: 0.0132175\n",
      "[106]\ttraining's l1: 0.0188982\tvalid_0's l1: 0.0132175\n",
      "[107]\ttraining's l1: 0.0188953\tvalid_0's l1: 0.0132172\n",
      "[108]\ttraining's l1: 0.0188926\tvalid_0's l1: 0.013217\n",
      "[109]\ttraining's l1: 0.0188898\tvalid_0's l1: 0.0132171\n",
      "[110]\ttraining's l1: 0.0188871\tvalid_0's l1: 0.0132169\n",
      "[111]\ttraining's l1: 0.0188847\tvalid_0's l1: 0.013216\n",
      "[112]\ttraining's l1: 0.0188824\tvalid_0's l1: 0.0132165\n",
      "[113]\ttraining's l1: 0.0188802\tvalid_0's l1: 0.0132173\n",
      "[114]\ttraining's l1: 0.0188779\tvalid_0's l1: 0.0132178\n",
      "[115]\ttraining's l1: 0.0188756\tvalid_0's l1: 0.0132186\n",
      "[116]\ttraining's l1: 0.0188737\tvalid_0's l1: 0.0132185\n",
      "[117]\ttraining's l1: 0.0188715\tvalid_0's l1: 0.0132185\n",
      "[118]\ttraining's l1: 0.0188695\tvalid_0's l1: 0.0132178\n",
      "[119]\ttraining's l1: 0.0188673\tvalid_0's l1: 0.0132172\n",
      "[120]\ttraining's l1: 0.0188653\tvalid_0's l1: 0.0132168\n",
      "[121]\ttraining's l1: 0.0188629\tvalid_0's l1: 0.0132166\n",
      "[122]\ttraining's l1: 0.0188599\tvalid_0's l1: 0.0132164\n",
      "[123]\ttraining's l1: 0.0188574\tvalid_0's l1: 0.0132169\n",
      "[124]\ttraining's l1: 0.0188547\tvalid_0's l1: 0.0132169\n",
      "[125]\ttraining's l1: 0.0188516\tvalid_0's l1: 0.0132166\n",
      "[126]\ttraining's l1: 0.0188496\tvalid_0's l1: 0.0132163\n",
      "[127]\ttraining's l1: 0.018847\tvalid_0's l1: 0.0132165\n",
      "[128]\ttraining's l1: 0.0188446\tvalid_0's l1: 0.0132164\n",
      "[129]\ttraining's l1: 0.0188426\tvalid_0's l1: 0.0132161\n",
      "[130]\ttraining's l1: 0.0188407\tvalid_0's l1: 0.0132161\n",
      "[131]\ttraining's l1: 0.0188381\tvalid_0's l1: 0.0132164\n",
      "[132]\ttraining's l1: 0.0188357\tvalid_0's l1: 0.0132168\n",
      "[133]\ttraining's l1: 0.0188335\tvalid_0's l1: 0.0132164\n",
      "[134]\ttraining's l1: 0.0188312\tvalid_0's l1: 0.0132165\n",
      "[135]\ttraining's l1: 0.0188288\tvalid_0's l1: 0.0132165\n",
      "[136]\ttraining's l1: 0.0188262\tvalid_0's l1: 0.0132163\n",
      "[137]\ttraining's l1: 0.0188237\tvalid_0's l1: 0.0132162\n",
      "[138]\ttraining's l1: 0.0188215\tvalid_0's l1: 0.0132161\n",
      "[139]\ttraining's l1: 0.018819\tvalid_0's l1: 0.013216\n",
      "[140]\ttraining's l1: 0.0188164\tvalid_0's l1: 0.013216\n",
      "[141]\ttraining's l1: 0.0188139\tvalid_0's l1: 0.013216\n",
      "[142]\ttraining's l1: 0.0188115\tvalid_0's l1: 0.0132159\n",
      "[143]\ttraining's l1: 0.0188091\tvalid_0's l1: 0.0132158\n",
      "[144]\ttraining's l1: 0.0188066\tvalid_0's l1: 0.0132156\n",
      "[145]\ttraining's l1: 0.0188039\tvalid_0's l1: 0.0132154\n",
      "[146]\ttraining's l1: 0.0188016\tvalid_0's l1: 0.013215\n",
      "[147]\ttraining's l1: 0.0187992\tvalid_0's l1: 0.0132154\n",
      "[148]\ttraining's l1: 0.0187969\tvalid_0's l1: 0.013215\n",
      "[149]\ttraining's l1: 0.0187948\tvalid_0's l1: 0.013215\n",
      "[150]\ttraining's l1: 0.0187925\tvalid_0's l1: 0.0132147\n",
      "[151]\ttraining's l1: 0.0187903\tvalid_0's l1: 0.0132143\n",
      "[152]\ttraining's l1: 0.0187879\tvalid_0's l1: 0.0132143\n",
      "[153]\ttraining's l1: 0.0187856\tvalid_0's l1: 0.0132145\n",
      "[154]\ttraining's l1: 0.0187833\tvalid_0's l1: 0.0132144\n",
      "[155]\ttraining's l1: 0.0187812\tvalid_0's l1: 0.013214\n",
      "[156]\ttraining's l1: 0.0187788\tvalid_0's l1: 0.0132139\n",
      "[157]\ttraining's l1: 0.0187767\tvalid_0's l1: 0.013214\n",
      "[158]\ttraining's l1: 0.0187747\tvalid_0's l1: 0.0132141\n",
      "[159]\ttraining's l1: 0.0187729\tvalid_0's l1: 0.0132147\n",
      "[160]\ttraining's l1: 0.018771\tvalid_0's l1: 0.0132144\n",
      "[161]\ttraining's l1: 0.0187685\tvalid_0's l1: 0.0132147\n",
      "[162]\ttraining's l1: 0.0187659\tvalid_0's l1: 0.0132155\n",
      "[163]\ttraining's l1: 0.0187633\tvalid_0's l1: 0.0132168\n",
      "[164]\ttraining's l1: 0.018761\tvalid_0's l1: 0.0132169\n",
      "[165]\ttraining's l1: 0.018759\tvalid_0's l1: 0.013217\n",
      "[166]\ttraining's l1: 0.0187567\tvalid_0's l1: 0.0132171\n",
      "[167]\ttraining's l1: 0.0187546\tvalid_0's l1: 0.0132171\n",
      "[168]\ttraining's l1: 0.0187525\tvalid_0's l1: 0.0132169\n",
      "[169]\ttraining's l1: 0.0187504\tvalid_0's l1: 0.0132171\n",
      "[170]\ttraining's l1: 0.018748\tvalid_0's l1: 0.0132171\n",
      "[171]\ttraining's l1: 0.0187457\tvalid_0's l1: 0.0132175\n",
      "[172]\ttraining's l1: 0.0187435\tvalid_0's l1: 0.0132177\n",
      "[173]\ttraining's l1: 0.0187413\tvalid_0's l1: 0.0132181\n",
      "[174]\ttraining's l1: 0.0187389\tvalid_0's l1: 0.0132185\n",
      "[175]\ttraining's l1: 0.0187366\tvalid_0's l1: 0.013219\n",
      "[176]\ttraining's l1: 0.0187339\tvalid_0's l1: 0.013219\n",
      "[177]\ttraining's l1: 0.0187316\tvalid_0's l1: 0.0132188\n",
      "[178]\ttraining's l1: 0.0187292\tvalid_0's l1: 0.0132187\n",
      "[179]\ttraining's l1: 0.0187268\tvalid_0's l1: 0.0132186\n",
      "[180]\ttraining's l1: 0.0187244\tvalid_0's l1: 0.0132184\n",
      "[181]\ttraining's l1: 0.0187224\tvalid_0's l1: 0.0132185\n",
      "[182]\ttraining's l1: 0.0187205\tvalid_0's l1: 0.0132182\n",
      "[183]\ttraining's l1: 0.0187183\tvalid_0's l1: 0.0132178\n",
      "[184]\ttraining's l1: 0.0187161\tvalid_0's l1: 0.0132178\n",
      "[185]\ttraining's l1: 0.0187135\tvalid_0's l1: 0.0132176\n",
      "[186]\ttraining's l1: 0.0187107\tvalid_0's l1: 0.0132178\n",
      "[187]\ttraining's l1: 0.0187078\tvalid_0's l1: 0.0132188\n",
      "[188]\ttraining's l1: 0.0187053\tvalid_0's l1: 0.0132189\n",
      "[189]\ttraining's l1: 0.0187025\tvalid_0's l1: 0.0132189\n",
      "[190]\ttraining's l1: 0.0186997\tvalid_0's l1: 0.0132191\n",
      "[191]\ttraining's l1: 0.018697\tvalid_0's l1: 0.0132197\n",
      "[192]\ttraining's l1: 0.0186945\tvalid_0's l1: 0.0132208\n",
      "[193]\ttraining's l1: 0.0186918\tvalid_0's l1: 0.0132212\n",
      "[194]\ttraining's l1: 0.0186891\tvalid_0's l1: 0.0132215\n",
      "[195]\ttraining's l1: 0.0186868\tvalid_0's l1: 0.0132216\n",
      "[196]\ttraining's l1: 0.0186843\tvalid_0's l1: 0.0132219\n",
      "[197]\ttraining's l1: 0.018682\tvalid_0's l1: 0.0132224\n",
      "[198]\ttraining's l1: 0.0186796\tvalid_0's l1: 0.0132224\n",
      "[199]\ttraining's l1: 0.0186771\tvalid_0's l1: 0.0132226\n",
      "[200]\ttraining's l1: 0.0186745\tvalid_0's l1: 0.0132228\n",
      "[201]\ttraining's l1: 0.018672\tvalid_0's l1: 0.0132232\n",
      "[202]\ttraining's l1: 0.0186696\tvalid_0's l1: 0.0132234\n",
      "[203]\ttraining's l1: 0.0186675\tvalid_0's l1: 0.0132235\n",
      "[204]\ttraining's l1: 0.0186651\tvalid_0's l1: 0.0132238\n",
      "[205]\ttraining's l1: 0.0186629\tvalid_0's l1: 0.013224\n",
      "[206]\ttraining's l1: 0.0186609\tvalid_0's l1: 0.0132242\n",
      "[207]\ttraining's l1: 0.0186591\tvalid_0's l1: 0.0132242\n",
      "[208]\ttraining's l1: 0.018657\tvalid_0's l1: 0.0132245\n",
      "[209]\ttraining's l1: 0.018655\tvalid_0's l1: 0.0132246\n",
      "[210]\ttraining's l1: 0.0186528\tvalid_0's l1: 0.0132253\n",
      "[211]\ttraining's l1: 0.0186508\tvalid_0's l1: 0.013226\n",
      "[212]\ttraining's l1: 0.0186488\tvalid_0's l1: 0.0132262\n",
      "[213]\ttraining's l1: 0.0186469\tvalid_0's l1: 0.0132266\n",
      "[214]\ttraining's l1: 0.0186449\tvalid_0's l1: 0.0132265\n",
      "[215]\ttraining's l1: 0.0186428\tvalid_0's l1: 0.0132267\n",
      "[216]\ttraining's l1: 0.0186405\tvalid_0's l1: 0.0132269\n",
      "[217]\ttraining's l1: 0.0186382\tvalid_0's l1: 0.0132269\n",
      "[218]\ttraining's l1: 0.0186358\tvalid_0's l1: 0.0132267\n",
      "[219]\ttraining's l1: 0.0186335\tvalid_0's l1: 0.0132264\n",
      "[220]\ttraining's l1: 0.0186312\tvalid_0's l1: 0.0132267\n",
      "[221]\ttraining's l1: 0.0186288\tvalid_0's l1: 0.0132268\n",
      "[222]\ttraining's l1: 0.0186269\tvalid_0's l1: 0.0132271\n",
      "[223]\ttraining's l1: 0.0186253\tvalid_0's l1: 0.0132276\n",
      "[224]\ttraining's l1: 0.0186234\tvalid_0's l1: 0.0132272\n",
      "[225]\ttraining's l1: 0.0186211\tvalid_0's l1: 0.0132273\n",
      "[226]\ttraining's l1: 0.0186188\tvalid_0's l1: 0.0132278\n",
      "[227]\ttraining's l1: 0.0186167\tvalid_0's l1: 0.0132282\n",
      "[228]\ttraining's l1: 0.0186147\tvalid_0's l1: 0.0132281\n",
      "[229]\ttraining's l1: 0.0186128\tvalid_0's l1: 0.0132283\n",
      "[230]\ttraining's l1: 0.0186103\tvalid_0's l1: 0.0132286\n",
      "[231]\ttraining's l1: 0.018608\tvalid_0's l1: 0.0132287\n",
      "[232]\ttraining's l1: 0.018606\tvalid_0's l1: 0.0132289\n",
      "[233]\ttraining's l1: 0.0186037\tvalid_0's l1: 0.0132298\n",
      "[234]\ttraining's l1: 0.0186011\tvalid_0's l1: 0.0132297\n",
      "[235]\ttraining's l1: 0.0185993\tvalid_0's l1: 0.0132298\n",
      "[236]\ttraining's l1: 0.0185976\tvalid_0's l1: 0.0132301\n",
      "[237]\ttraining's l1: 0.0185956\tvalid_0's l1: 0.0132302\n",
      "[238]\ttraining's l1: 0.0185938\tvalid_0's l1: 0.0132308\n",
      "[239]\ttraining's l1: 0.0185917\tvalid_0's l1: 0.013231\n",
      "[240]\ttraining's l1: 0.0185901\tvalid_0's l1: 0.013231\n",
      "[241]\ttraining's l1: 0.0185883\tvalid_0's l1: 0.013231\n",
      "[242]\ttraining's l1: 0.0185863\tvalid_0's l1: 0.0132316\n",
      "[243]\ttraining's l1: 0.0185839\tvalid_0's l1: 0.0132314\n",
      "[244]\ttraining's l1: 0.0185818\tvalid_0's l1: 0.0132319\n",
      "[245]\ttraining's l1: 0.01858\tvalid_0's l1: 0.0132325\n",
      "[246]\ttraining's l1: 0.0185782\tvalid_0's l1: 0.013233\n",
      "[247]\ttraining's l1: 0.0185763\tvalid_0's l1: 0.0132334\n",
      "[248]\ttraining's l1: 0.0185747\tvalid_0's l1: 0.0132339\n",
      "[249]\ttraining's l1: 0.018573\tvalid_0's l1: 0.0132344\n",
      "[250]\ttraining's l1: 0.0185713\tvalid_0's l1: 0.0132346\n",
      "[251]\ttraining's l1: 0.0185699\tvalid_0's l1: 0.0132347\n",
      "[252]\ttraining's l1: 0.0185677\tvalid_0's l1: 0.0132345\n",
      "[253]\ttraining's l1: 0.0185657\tvalid_0's l1: 0.013235\n",
      "[254]\ttraining's l1: 0.0185636\tvalid_0's l1: 0.0132355\n",
      "[255]\ttraining's l1: 0.0185614\tvalid_0's l1: 0.0132356\n",
      "[256]\ttraining's l1: 0.0185598\tvalid_0's l1: 0.0132356\n",
      "[257]\ttraining's l1: 0.0185581\tvalid_0's l1: 0.0132356\n",
      "[258]\ttraining's l1: 0.0185565\tvalid_0's l1: 0.0132357\n",
      "[259]\ttraining's l1: 0.0185547\tvalid_0's l1: 0.0132358\n",
      "[260]\ttraining's l1: 0.0185528\tvalid_0's l1: 0.0132359\n",
      "[261]\ttraining's l1: 0.0185509\tvalid_0's l1: 0.0132358\n",
      "[262]\ttraining's l1: 0.0185489\tvalid_0's l1: 0.0132361\n",
      "[263]\ttraining's l1: 0.018547\tvalid_0's l1: 0.0132369\n",
      "[264]\ttraining's l1: 0.0185449\tvalid_0's l1: 0.0132375\n",
      "[265]\ttraining's l1: 0.0185428\tvalid_0's l1: 0.0132379\n",
      "[266]\ttraining's l1: 0.0185407\tvalid_0's l1: 0.0132382\n",
      "[267]\ttraining's l1: 0.018539\tvalid_0's l1: 0.0132392\n",
      "[268]\ttraining's l1: 0.0185371\tvalid_0's l1: 0.0132396\n",
      "[269]\ttraining's l1: 0.0185352\tvalid_0's l1: 0.0132399\n",
      "[270]\ttraining's l1: 0.0185332\tvalid_0's l1: 0.0132405\n",
      "[271]\ttraining's l1: 0.0185309\tvalid_0's l1: 0.0132407\n",
      "[272]\ttraining's l1: 0.0185285\tvalid_0's l1: 0.013241\n",
      "[273]\ttraining's l1: 0.0185259\tvalid_0's l1: 0.013241\n",
      "[274]\ttraining's l1: 0.0185235\tvalid_0's l1: 0.0132407\n",
      "[275]\ttraining's l1: 0.018521\tvalid_0's l1: 0.0132408\n",
      "[276]\ttraining's l1: 0.0185186\tvalid_0's l1: 0.0132409\n",
      "[277]\ttraining's l1: 0.0185162\tvalid_0's l1: 0.0132409\n",
      "[278]\ttraining's l1: 0.0185139\tvalid_0's l1: 0.013241\n",
      "[279]\ttraining's l1: 0.0185113\tvalid_0's l1: 0.0132413\n",
      "[280]\ttraining's l1: 0.018509\tvalid_0's l1: 0.013242\n",
      "[281]\ttraining's l1: 0.0185068\tvalid_0's l1: 0.0132412\n",
      "[282]\ttraining's l1: 0.0185047\tvalid_0's l1: 0.0132405\n",
      "[283]\ttraining's l1: 0.0185026\tvalid_0's l1: 0.0132411\n",
      "[284]\ttraining's l1: 0.0185004\tvalid_0's l1: 0.0132424\n",
      "[285]\ttraining's l1: 0.0184983\tvalid_0's l1: 0.0132436\n",
      "[286]\ttraining's l1: 0.0184959\tvalid_0's l1: 0.0132435\n",
      "[287]\ttraining's l1: 0.0184938\tvalid_0's l1: 0.0132435\n",
      "[288]\ttraining's l1: 0.0184915\tvalid_0's l1: 0.0132438\n",
      "[289]\ttraining's l1: 0.0184897\tvalid_0's l1: 0.0132444\n",
      "[290]\ttraining's l1: 0.0184874\tvalid_0's l1: 0.0132445\n",
      "[291]\ttraining's l1: 0.0184851\tvalid_0's l1: 0.0132446\n",
      "[292]\ttraining's l1: 0.0184832\tvalid_0's l1: 0.0132449\n",
      "[293]\ttraining's l1: 0.0184815\tvalid_0's l1: 0.0132444\n",
      "[294]\ttraining's l1: 0.0184797\tvalid_0's l1: 0.0132449\n",
      "[295]\ttraining's l1: 0.0184774\tvalid_0's l1: 0.0132448\n",
      "[296]\ttraining's l1: 0.0184758\tvalid_0's l1: 0.0132455\n",
      "[297]\ttraining's l1: 0.0184738\tvalid_0's l1: 0.0132464\n",
      "[298]\ttraining's l1: 0.018472\tvalid_0's l1: 0.0132463\n",
      "[299]\ttraining's l1: 0.0184701\tvalid_0's l1: 0.0132469\n",
      "[300]\ttraining's l1: 0.0184683\tvalid_0's l1: 0.0132473\n",
      "[301]\ttraining's l1: 0.0184664\tvalid_0's l1: 0.0132473\n",
      "[302]\ttraining's l1: 0.0184647\tvalid_0's l1: 0.0132475\n",
      "[303]\ttraining's l1: 0.0184629\tvalid_0's l1: 0.0132478\n",
      "[304]\ttraining's l1: 0.0184612\tvalid_0's l1: 0.0132478\n",
      "[305]\ttraining's l1: 0.0184593\tvalid_0's l1: 0.0132477\n",
      "[306]\ttraining's l1: 0.018457\tvalid_0's l1: 0.013248\n",
      "[307]\ttraining's l1: 0.0184548\tvalid_0's l1: 0.0132481\n",
      "[308]\ttraining's l1: 0.0184526\tvalid_0's l1: 0.0132482\n",
      "[309]\ttraining's l1: 0.01845\tvalid_0's l1: 0.0132486\n",
      "[310]\ttraining's l1: 0.0184476\tvalid_0's l1: 0.013249\n",
      "[311]\ttraining's l1: 0.0184456\tvalid_0's l1: 0.013249\n",
      "[312]\ttraining's l1: 0.0184437\tvalid_0's l1: 0.0132495\n",
      "[313]\ttraining's l1: 0.0184422\tvalid_0's l1: 0.0132497\n",
      "[314]\ttraining's l1: 0.0184405\tvalid_0's l1: 0.0132502\n",
      "[315]\ttraining's l1: 0.0184386\tvalid_0's l1: 0.0132503\n",
      "[316]\ttraining's l1: 0.0184362\tvalid_0's l1: 0.0132503\n",
      "[317]\ttraining's l1: 0.018434\tvalid_0's l1: 0.0132502\n",
      "[318]\ttraining's l1: 0.0184315\tvalid_0's l1: 0.0132502\n",
      "[319]\ttraining's l1: 0.0184293\tvalid_0's l1: 0.0132501\n",
      "[320]\ttraining's l1: 0.0184273\tvalid_0's l1: 0.01325\n",
      "[321]\ttraining's l1: 0.0184256\tvalid_0's l1: 0.0132503\n",
      "[322]\ttraining's l1: 0.0184238\tvalid_0's l1: 0.0132506\n",
      "[323]\ttraining's l1: 0.0184221\tvalid_0's l1: 0.0132508\n",
      "[324]\ttraining's l1: 0.0184203\tvalid_0's l1: 0.0132509\n",
      "[325]\ttraining's l1: 0.0184188\tvalid_0's l1: 0.0132507\n",
      "[326]\ttraining's l1: 0.0184172\tvalid_0's l1: 0.0132513\n",
      "[327]\ttraining's l1: 0.0184158\tvalid_0's l1: 0.0132513\n",
      "[328]\ttraining's l1: 0.0184144\tvalid_0's l1: 0.0132515\n",
      "[329]\ttraining's l1: 0.0184125\tvalid_0's l1: 0.013252\n",
      "[330]\ttraining's l1: 0.0184108\tvalid_0's l1: 0.0132525\n",
      "[331]\ttraining's l1: 0.0184087\tvalid_0's l1: 0.0132525\n",
      "[332]\ttraining's l1: 0.0184068\tvalid_0's l1: 0.0132532\n",
      "[333]\ttraining's l1: 0.0184049\tvalid_0's l1: 0.0132538\n",
      "[334]\ttraining's l1: 0.0184033\tvalid_0's l1: 0.0132547\n",
      "[335]\ttraining's l1: 0.0184012\tvalid_0's l1: 0.0132541\n",
      "[336]\ttraining's l1: 0.018399\tvalid_0's l1: 0.0132546\n",
      "[337]\ttraining's l1: 0.018397\tvalid_0's l1: 0.0132546\n",
      "[338]\ttraining's l1: 0.0183954\tvalid_0's l1: 0.0132545\n",
      "[339]\ttraining's l1: 0.0183939\tvalid_0's l1: 0.0132544\n",
      "[340]\ttraining's l1: 0.0183924\tvalid_0's l1: 0.0132548\n",
      "[341]\ttraining's l1: 0.0183905\tvalid_0's l1: 0.0132552\n",
      "[342]\ttraining's l1: 0.0183885\tvalid_0's l1: 0.0132565\n",
      "[343]\ttraining's l1: 0.0183864\tvalid_0's l1: 0.0132572\n",
      "[344]\ttraining's l1: 0.0183847\tvalid_0's l1: 0.0132574\n",
      "[345]\ttraining's l1: 0.0183832\tvalid_0's l1: 0.0132574\n",
      "[346]\ttraining's l1: 0.0183817\tvalid_0's l1: 0.0132576\n",
      "[347]\ttraining's l1: 0.0183799\tvalid_0's l1: 0.0132578\n",
      "[348]\ttraining's l1: 0.0183783\tvalid_0's l1: 0.0132584\n",
      "[349]\ttraining's l1: 0.0183765\tvalid_0's l1: 0.0132587\n",
      "[350]\ttraining's l1: 0.0183748\tvalid_0's l1: 0.0132589\n",
      "[351]\ttraining's l1: 0.0183727\tvalid_0's l1: 0.0132586\n",
      "[352]\ttraining's l1: 0.0183709\tvalid_0's l1: 0.0132585\n",
      "[353]\ttraining's l1: 0.018369\tvalid_0's l1: 0.0132597\n",
      "[354]\ttraining's l1: 0.0183671\tvalid_0's l1: 0.0132602\n",
      "[355]\ttraining's l1: 0.0183651\tvalid_0's l1: 0.0132603\n",
      "[356]\ttraining's l1: 0.0183633\tvalid_0's l1: 0.0132608\n",
      "[357]\ttraining's l1: 0.0183614\tvalid_0's l1: 0.0132609\n",
      "[358]\ttraining's l1: 0.0183596\tvalid_0's l1: 0.0132614\n",
      "[359]\ttraining's l1: 0.0183579\tvalid_0's l1: 0.0132614\n",
      "[360]\ttraining's l1: 0.0183561\tvalid_0's l1: 0.0132617\n",
      "[361]\ttraining's l1: 0.0183543\tvalid_0's l1: 0.0132618\n",
      "[362]\ttraining's l1: 0.0183523\tvalid_0's l1: 0.0132612\n",
      "[363]\ttraining's l1: 0.0183503\tvalid_0's l1: 0.0132614\n",
      "[364]\ttraining's l1: 0.0183485\tvalid_0's l1: 0.013261\n",
      "[365]\ttraining's l1: 0.0183467\tvalid_0's l1: 0.0132606\n",
      "[366]\ttraining's l1: 0.0183448\tvalid_0's l1: 0.0132609\n",
      "[367]\ttraining's l1: 0.0183433\tvalid_0's l1: 0.0132611\n",
      "[368]\ttraining's l1: 0.0183415\tvalid_0's l1: 0.0132621\n",
      "[369]\ttraining's l1: 0.0183397\tvalid_0's l1: 0.0132624\n",
      "[370]\ttraining's l1: 0.0183379\tvalid_0's l1: 0.0132624\n",
      "[371]\ttraining's l1: 0.0183359\tvalid_0's l1: 0.0132632\n",
      "[372]\ttraining's l1: 0.0183344\tvalid_0's l1: 0.0132634\n",
      "[373]\ttraining's l1: 0.0183327\tvalid_0's l1: 0.0132636\n",
      "[374]\ttraining's l1: 0.0183306\tvalid_0's l1: 0.013264\n",
      "[375]\ttraining's l1: 0.0183291\tvalid_0's l1: 0.0132643\n",
      "[376]\ttraining's l1: 0.0183271\tvalid_0's l1: 0.0132644\n",
      "[377]\ttraining's l1: 0.0183245\tvalid_0's l1: 0.013265\n",
      "[378]\ttraining's l1: 0.0183223\tvalid_0's l1: 0.0132669\n",
      "[379]\ttraining's l1: 0.0183202\tvalid_0's l1: 0.013267\n",
      "[380]\ttraining's l1: 0.0183183\tvalid_0's l1: 0.0132675\n",
      "[381]\ttraining's l1: 0.0183162\tvalid_0's l1: 0.0132679\n",
      "[382]\ttraining's l1: 0.0183144\tvalid_0's l1: 0.013268\n",
      "[383]\ttraining's l1: 0.0183127\tvalid_0's l1: 0.0132687\n",
      "[384]\ttraining's l1: 0.0183107\tvalid_0's l1: 0.0132689\n",
      "[385]\ttraining's l1: 0.0183087\tvalid_0's l1: 0.0132701\n",
      "[386]\ttraining's l1: 0.0183068\tvalid_0's l1: 0.0132699\n",
      "[387]\ttraining's l1: 0.0183051\tvalid_0's l1: 0.0132701\n",
      "[388]\ttraining's l1: 0.018303\tvalid_0's l1: 0.0132702\n",
      "[389]\ttraining's l1: 0.0183013\tvalid_0's l1: 0.0132707\n",
      "[390]\ttraining's l1: 0.0182995\tvalid_0's l1: 0.0132706\n",
      "[391]\ttraining's l1: 0.0182975\tvalid_0's l1: 0.0132708\n",
      "[392]\ttraining's l1: 0.0182956\tvalid_0's l1: 0.0132711\n",
      "[393]\ttraining's l1: 0.0182939\tvalid_0's l1: 0.0132714\n",
      "[394]\ttraining's l1: 0.0182919\tvalid_0's l1: 0.0132719\n",
      "[395]\ttraining's l1: 0.0182904\tvalid_0's l1: 0.0132717\n",
      "[396]\ttraining's l1: 0.0182886\tvalid_0's l1: 0.0132718\n",
      "[397]\ttraining's l1: 0.0182868\tvalid_0's l1: 0.0132723\n",
      "[398]\ttraining's l1: 0.0182849\tvalid_0's l1: 0.0132724\n",
      "[399]\ttraining's l1: 0.0182831\tvalid_0's l1: 0.0132724\n",
      "[400]\ttraining's l1: 0.018281\tvalid_0's l1: 0.0132727\n",
      "[401]\ttraining's l1: 0.0182792\tvalid_0's l1: 0.0132728\n",
      "[402]\ttraining's l1: 0.0182771\tvalid_0's l1: 0.0132738\n",
      "[403]\ttraining's l1: 0.0182757\tvalid_0's l1: 0.0132736\n",
      "[404]\ttraining's l1: 0.0182738\tvalid_0's l1: 0.013274\n",
      "[405]\ttraining's l1: 0.0182718\tvalid_0's l1: 0.0132745\n",
      "[406]\ttraining's l1: 0.0182696\tvalid_0's l1: 0.0132748\n",
      "[407]\ttraining's l1: 0.0182677\tvalid_0's l1: 0.0132752\n",
      "[408]\ttraining's l1: 0.0182657\tvalid_0's l1: 0.0132757\n",
      "[409]\ttraining's l1: 0.0182638\tvalid_0's l1: 0.0132756\n",
      "[410]\ttraining's l1: 0.0182616\tvalid_0's l1: 0.0132764\n",
      "[411]\ttraining's l1: 0.0182598\tvalid_0's l1: 0.0132767\n",
      "[412]\ttraining's l1: 0.018258\tvalid_0's l1: 0.0132769\n",
      "[413]\ttraining's l1: 0.018256\tvalid_0's l1: 0.0132781\n",
      "[414]\ttraining's l1: 0.0182542\tvalid_0's l1: 0.0132785\n",
      "[415]\ttraining's l1: 0.0182523\tvalid_0's l1: 0.0132785\n",
      "[416]\ttraining's l1: 0.0182504\tvalid_0's l1: 0.0132791\n",
      "[417]\ttraining's l1: 0.0182485\tvalid_0's l1: 0.0132793\n",
      "[418]\ttraining's l1: 0.0182463\tvalid_0's l1: 0.0132798\n",
      "[419]\ttraining's l1: 0.0182441\tvalid_0's l1: 0.0132804\n",
      "[420]\ttraining's l1: 0.018242\tvalid_0's l1: 0.0132808\n",
      "[421]\ttraining's l1: 0.0182401\tvalid_0's l1: 0.0132816\n",
      "[422]\ttraining's l1: 0.0182381\tvalid_0's l1: 0.0132816\n",
      "[423]\ttraining's l1: 0.0182367\tvalid_0's l1: 0.0132824\n",
      "[424]\ttraining's l1: 0.0182354\tvalid_0's l1: 0.0132824\n",
      "[425]\ttraining's l1: 0.0182336\tvalid_0's l1: 0.0132832\n",
      "[426]\ttraining's l1: 0.0182318\tvalid_0's l1: 0.0132834\n",
      "[427]\ttraining's l1: 0.0182303\tvalid_0's l1: 0.0132835\n",
      "[428]\ttraining's l1: 0.0182285\tvalid_0's l1: 0.0132841\n",
      "[429]\ttraining's l1: 0.0182266\tvalid_0's l1: 0.0132843\n",
      "[430]\ttraining's l1: 0.018225\tvalid_0's l1: 0.0132856\n",
      "[431]\ttraining's l1: 0.0182234\tvalid_0's l1: 0.0132857\n",
      "[432]\ttraining's l1: 0.0182216\tvalid_0's l1: 0.0132862\n",
      "[433]\ttraining's l1: 0.0182194\tvalid_0's l1: 0.0132866\n",
      "[434]\ttraining's l1: 0.0182179\tvalid_0's l1: 0.0132873\n",
      "[435]\ttraining's l1: 0.0182164\tvalid_0's l1: 0.0132874\n",
      "[436]\ttraining's l1: 0.0182145\tvalid_0's l1: 0.0132884\n",
      "[437]\ttraining's l1: 0.0182127\tvalid_0's l1: 0.0132889\n",
      "[438]\ttraining's l1: 0.0182113\tvalid_0's l1: 0.0132894\n",
      "[439]\ttraining's l1: 0.0182097\tvalid_0's l1: 0.0132898\n",
      "[440]\ttraining's l1: 0.0182085\tvalid_0's l1: 0.0132906\n",
      "[441]\ttraining's l1: 0.0182068\tvalid_0's l1: 0.0132907\n",
      "[442]\ttraining's l1: 0.0182052\tvalid_0's l1: 0.0132913\n",
      "[443]\ttraining's l1: 0.0182037\tvalid_0's l1: 0.0132926\n",
      "[444]\ttraining's l1: 0.0182022\tvalid_0's l1: 0.0132939\n",
      "[445]\ttraining's l1: 0.0182006\tvalid_0's l1: 0.0132951\n",
      "[446]\ttraining's l1: 0.018199\tvalid_0's l1: 0.0132954\n",
      "[447]\ttraining's l1: 0.0181974\tvalid_0's l1: 0.0132957\n",
      "[448]\ttraining's l1: 0.0181956\tvalid_0's l1: 0.0132963\n",
      "[449]\ttraining's l1: 0.0181941\tvalid_0's l1: 0.0132966\n",
      "[450]\ttraining's l1: 0.0181924\tvalid_0's l1: 0.013297\n",
      "[451]\ttraining's l1: 0.0181904\tvalid_0's l1: 0.0132973\n",
      "[452]\ttraining's l1: 0.0181888\tvalid_0's l1: 0.0132983\n",
      "[453]\ttraining's l1: 0.0181873\tvalid_0's l1: 0.013299\n",
      "[454]\ttraining's l1: 0.0181853\tvalid_0's l1: 0.0133001\n",
      "[455]\ttraining's l1: 0.0181834\tvalid_0's l1: 0.0133013\n",
      "[456]\ttraining's l1: 0.0181814\tvalid_0's l1: 0.0133023\n",
      "[457]\ttraining's l1: 0.0181793\tvalid_0's l1: 0.0133031\n",
      "[458]\ttraining's l1: 0.0181777\tvalid_0's l1: 0.0133037\n",
      "[459]\ttraining's l1: 0.0181761\tvalid_0's l1: 0.0133037\n",
      "[460]\ttraining's l1: 0.018174\tvalid_0's l1: 0.0133047\n",
      "[461]\ttraining's l1: 0.0181723\tvalid_0's l1: 0.0133055\n",
      "[462]\ttraining's l1: 0.0181707\tvalid_0's l1: 0.0133063\n",
      "[463]\ttraining's l1: 0.0181688\tvalid_0's l1: 0.0133065\n",
      "[464]\ttraining's l1: 0.0181668\tvalid_0's l1: 0.0133074\n",
      "[465]\ttraining's l1: 0.0181648\tvalid_0's l1: 0.0133091\n",
      "[466]\ttraining's l1: 0.0181632\tvalid_0's l1: 0.0133099\n",
      "[467]\ttraining's l1: 0.0181613\tvalid_0's l1: 0.0133108\n",
      "[468]\ttraining's l1: 0.0181597\tvalid_0's l1: 0.0133117\n",
      "[469]\ttraining's l1: 0.0181578\tvalid_0's l1: 0.0133125\n",
      "[470]\ttraining's l1: 0.0181562\tvalid_0's l1: 0.0133129\n",
      "[471]\ttraining's l1: 0.0181542\tvalid_0's l1: 0.0133137\n",
      "[472]\ttraining's l1: 0.0181525\tvalid_0's l1: 0.0133136\n",
      "[473]\ttraining's l1: 0.0181507\tvalid_0's l1: 0.0133138\n",
      "[474]\ttraining's l1: 0.0181489\tvalid_0's l1: 0.013315\n",
      "[475]\ttraining's l1: 0.0181476\tvalid_0's l1: 0.0133152\n",
      "[476]\ttraining's l1: 0.0181457\tvalid_0's l1: 0.0133155\n",
      "[477]\ttraining's l1: 0.0181442\tvalid_0's l1: 0.0133159\n",
      "[478]\ttraining's l1: 0.018142\tvalid_0's l1: 0.0133162\n",
      "[479]\ttraining's l1: 0.0181405\tvalid_0's l1: 0.0133165\n",
      "[480]\ttraining's l1: 0.0181384\tvalid_0's l1: 0.0133177\n",
      "[481]\ttraining's l1: 0.0181368\tvalid_0's l1: 0.0133173\n",
      "[482]\ttraining's l1: 0.0181351\tvalid_0's l1: 0.0133179\n",
      "[483]\ttraining's l1: 0.0181329\tvalid_0's l1: 0.0133188\n",
      "[484]\ttraining's l1: 0.0181311\tvalid_0's l1: 0.0133198\n",
      "[485]\ttraining's l1: 0.0181289\tvalid_0's l1: 0.0133213\n",
      "[486]\ttraining's l1: 0.018127\tvalid_0's l1: 0.0133215\n",
      "[487]\ttraining's l1: 0.0181252\tvalid_0's l1: 0.0133223\n",
      "[488]\ttraining's l1: 0.0181233\tvalid_0's l1: 0.0133233\n",
      "[489]\ttraining's l1: 0.0181215\tvalid_0's l1: 0.0133244\n",
      "[490]\ttraining's l1: 0.0181202\tvalid_0's l1: 0.0133247\n",
      "[491]\ttraining's l1: 0.0181185\tvalid_0's l1: 0.0133257\n",
      "[492]\ttraining's l1: 0.0181172\tvalid_0's l1: 0.0133256\n",
      "[493]\ttraining's l1: 0.0181157\tvalid_0's l1: 0.0133263\n",
      "[494]\ttraining's l1: 0.0181141\tvalid_0's l1: 0.0133272\n",
      "[495]\ttraining's l1: 0.0181126\tvalid_0's l1: 0.0133275\n",
      "[496]\ttraining's l1: 0.018111\tvalid_0's l1: 0.0133281\n",
      "[497]\ttraining's l1: 0.018109\tvalid_0's l1: 0.0133288\n",
      "[498]\ttraining's l1: 0.0181074\tvalid_0's l1: 0.0133289\n",
      "[499]\ttraining's l1: 0.0181056\tvalid_0's l1: 0.0133291\n",
      "[500]\ttraining's l1: 0.018104\tvalid_0's l1: 0.0133294\n",
      "[501]\ttraining's l1: 0.018102\tvalid_0's l1: 0.0133302\n",
      "[502]\ttraining's l1: 0.0180999\tvalid_0's l1: 0.0133313\n",
      "[503]\ttraining's l1: 0.0180978\tvalid_0's l1: 0.0133328\n",
      "[504]\ttraining's l1: 0.018096\tvalid_0's l1: 0.0133326\n",
      "[505]\ttraining's l1: 0.0180942\tvalid_0's l1: 0.0133332\n",
      "[506]\ttraining's l1: 0.0180923\tvalid_0's l1: 0.0133347\n",
      "[507]\ttraining's l1: 0.0180907\tvalid_0's l1: 0.0133352\n",
      "[508]\ttraining's l1: 0.0180889\tvalid_0's l1: 0.0133356\n",
      "[509]\ttraining's l1: 0.0180875\tvalid_0's l1: 0.0133356\n",
      "[510]\ttraining's l1: 0.0180856\tvalid_0's l1: 0.0133356\n",
      "[511]\ttraining's l1: 0.0180842\tvalid_0's l1: 0.0133357\n",
      "[512]\ttraining's l1: 0.0180826\tvalid_0's l1: 0.0133367\n",
      "[513]\ttraining's l1: 0.018081\tvalid_0's l1: 0.0133367\n",
      "[514]\ttraining's l1: 0.0180795\tvalid_0's l1: 0.0133368\n",
      "[515]\ttraining's l1: 0.0180781\tvalid_0's l1: 0.0133372\n",
      "[516]\ttraining's l1: 0.018077\tvalid_0's l1: 0.0133382\n",
      "[517]\ttraining's l1: 0.0180752\tvalid_0's l1: 0.013339\n",
      "[518]\ttraining's l1: 0.0180735\tvalid_0's l1: 0.0133404\n",
      "[519]\ttraining's l1: 0.0180718\tvalid_0's l1: 0.013341\n",
      "[520]\ttraining's l1: 0.0180703\tvalid_0's l1: 0.0133422\n",
      "[521]\ttraining's l1: 0.0180684\tvalid_0's l1: 0.0133433\n",
      "[522]\ttraining's l1: 0.018067\tvalid_0's l1: 0.0133436\n",
      "[523]\ttraining's l1: 0.0180657\tvalid_0's l1: 0.0133448\n",
      "[524]\ttraining's l1: 0.0180641\tvalid_0's l1: 0.0133451\n",
      "[525]\ttraining's l1: 0.0180628\tvalid_0's l1: 0.0133462\n",
      "[526]\ttraining's l1: 0.0180608\tvalid_0's l1: 0.0133471\n",
      "[527]\ttraining's l1: 0.0180594\tvalid_0's l1: 0.0133471\n",
      "[528]\ttraining's l1: 0.0180575\tvalid_0's l1: 0.0133476\n",
      "[529]\ttraining's l1: 0.0180559\tvalid_0's l1: 0.0133491\n",
      "[530]\ttraining's l1: 0.0180543\tvalid_0's l1: 0.0133499\n",
      "[531]\ttraining's l1: 0.0180529\tvalid_0's l1: 0.0133503\n",
      "[532]\ttraining's l1: 0.0180514\tvalid_0's l1: 0.0133507\n",
      "[533]\ttraining's l1: 0.0180498\tvalid_0's l1: 0.0133511\n",
      "[534]\ttraining's l1: 0.018048\tvalid_0's l1: 0.0133516\n",
      "[535]\ttraining's l1: 0.0180464\tvalid_0's l1: 0.0133522\n",
      "[536]\ttraining's l1: 0.0180442\tvalid_0's l1: 0.013352\n",
      "[537]\ttraining's l1: 0.0180425\tvalid_0's l1: 0.0133527\n",
      "[538]\ttraining's l1: 0.0180404\tvalid_0's l1: 0.013353\n",
      "[539]\ttraining's l1: 0.0180383\tvalid_0's l1: 0.0133537\n",
      "[540]\ttraining's l1: 0.0180362\tvalid_0's l1: 0.0133546\n",
      "[541]\ttraining's l1: 0.0180346\tvalid_0's l1: 0.0133553\n",
      "[542]\ttraining's l1: 0.0180331\tvalid_0's l1: 0.0133552\n",
      "[543]\ttraining's l1: 0.0180314\tvalid_0's l1: 0.0133555\n",
      "[544]\ttraining's l1: 0.0180296\tvalid_0's l1: 0.0133551\n",
      "[545]\ttraining's l1: 0.0180276\tvalid_0's l1: 0.0133551\n",
      "[546]\ttraining's l1: 0.0180259\tvalid_0's l1: 0.0133555\n",
      "[547]\ttraining's l1: 0.0180245\tvalid_0's l1: 0.0133552\n",
      "[548]\ttraining's l1: 0.0180231\tvalid_0's l1: 0.0133549\n",
      "[549]\ttraining's l1: 0.0180216\tvalid_0's l1: 0.0133548\n",
      "[550]\ttraining's l1: 0.0180198\tvalid_0's l1: 0.0133551\n",
      "[551]\ttraining's l1: 0.0180179\tvalid_0's l1: 0.0133562\n",
      "[552]\ttraining's l1: 0.018016\tvalid_0's l1: 0.0133565\n",
      "[553]\ttraining's l1: 0.0180146\tvalid_0's l1: 0.0133569\n",
      "[554]\ttraining's l1: 0.018013\tvalid_0's l1: 0.0133586\n",
      "[555]\ttraining's l1: 0.0180113\tvalid_0's l1: 0.0133604\n",
      "[556]\ttraining's l1: 0.0180096\tvalid_0's l1: 0.0133606\n",
      "Early stopping, best iteration is:\n",
      "[156]\ttraining's l1: 0.0187788\tvalid_0's l1: 0.0132139\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.001,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'num_threads':40\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train,Y_train)\n",
    "lgb_eval = [lgb.Dataset(X_test,Y_test,reference=lgb_train),lgb_train]\n",
    "model=lgb.train(params,lgb_train,num_boost_round=3000,valid_sets=lgb_eval,early_stopping_rounds=400)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353008.97959183675\n",
      "145956.15940147487\n",
      "347687.02040816325\n",
      "2.382133250381004\n"
     ]
    }
   ],
   "source": [
    "b=dataset.loc[(slice(None),'000002'),:]\n",
    "vvv=b['volume']\n",
    "print(vvv.mean())\n",
    "print(vvv.std())\n",
    "\n",
    "m=700696-vvv.mean()\n",
    "print(m)\n",
    "s=m/vvv.std()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockCode=QA.QA_fetch_stock_list()['code'].tolist()\n",
    "data = QA.QA_fetch_stock_day_adv(stockCode, '2018-03-20', '2018-08-23')\n",
    "#print(stockCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at least 2 positional arguments (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b08d5b8a6120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mAccount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQA_Account\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mBroker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQA_BacktestBroker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mQUANTAXIS\\QAARP\\QAAccount.pyx\u001b[0m in \u001b[0;36mQUANTAXIS.QAARP.QAAccount.QA_Account.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes at least 2 positional arguments (0 given)"
     ]
    }
   ],
   "source": [
    "# data.data\n",
    "# Account=QA.QA_Account()\n",
    "# Broker=QA.QA_BacktestBroker()\n",
    "user = QA.QA_user(username ='quantaxis', password = 'quantaxis')\n",
    "portfotlio=user.new_portfolio('x1')\n",
    "account = portfolio.new_account(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

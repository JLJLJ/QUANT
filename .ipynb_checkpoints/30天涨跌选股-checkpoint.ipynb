{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are using non-interactive mdoel quantaxis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiang/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:172: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/home/jiang/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:172: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "import QUANTAXIS as QA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import  pprint as print\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def func_scale(df,groups):\n",
    "    #部分不同列的数据具有相关性，需要统一标准化\n",
    "    for group in groups:\n",
    "        if type(group)!=list and type(group)!=tuple:\n",
    "            group=[group]\n",
    "        assemble_scale_column=group\n",
    "        df[assemble_scale_column] = preprocessing.scale(np.ravel(df[assemble_scale_column])).reshape([len(df),-1])\n",
    "    return df\n",
    "def standardization(df,groups):\n",
    "    #对数据去中心化，使符合正态分布\n",
    "    df=df.groupby('code').apply(func_scale,groups)\n",
    "    return df\n",
    "\n",
    "def func_series_to_supervised(dataset, n_in=1):\n",
    "    #dataset = pd.DataFrame(data)\n",
    "    df_all=pd.DataFrame()\n",
    "    \n",
    "    for i in range(n_in, 0, -1):\n",
    "        df_back=dataset.shift(i)\n",
    "        col_names=df_back.columns+'_'+str(i)\n",
    "        df_back.columns=col_names\n",
    "        df_all=pd.concat([df_all,df_back],axis=1)\n",
    "        #print(df_all.index)\n",
    "    return df_all\n",
    "#series_to_supervised(dataset,3)\n",
    "def series_to_supervised(dataset, n_in=1):\n",
    "    return dataset.groupby('code').apply(func_series_to_supervised,n_in)\n",
    "\n",
    "def max_min_close_n_days(data, n=30):\n",
    "    '''\n",
    "    自定义指标，计算当日（不含）到n天后收盘价最大值、最小值、最大值上涨比率、最小值下跌比率\n",
    "    '''\n",
    "    n_days_max=pd.Series(np.zeros(len(data)))\n",
    "    n_days_min=pd.Series(np.zeros(len(data)))\n",
    "    n_days_max_radio=pd.Series(np.zeros(len(data)))\n",
    "    n_days_min_radio=pd.Series(np.zeros(len(data)))\n",
    "    if(len(data)-n<0):\n",
    "        '''for i in range(len(data)):\n",
    "            n_days_max[i]=None\n",
    "            n_days_min[i]=None'''\n",
    "        n_days_max[0:len(data)]=None\n",
    "        n_days_min[0:len(data)]=None\n",
    "    else:\n",
    "        for i in range(len(data)-n):\n",
    "            n_days_max[i]=(max(data['close'][i+1:i+n+1]))\n",
    "            n_days_min[i]=(min(data['close'][i+1:i+n+1]))\n",
    "            n_days_max_radio[i]=(n_days_max[i]-data['close'][i])/data['close'][i]\n",
    "            n_days_min_radio[i]=(n_days_min[i]-data['close'][i])/data['close'][i]\n",
    "        n_days_max[len(data)-n:len(data)]=None\n",
    "        n_days_min[len(data)-n:len(data)]=None\n",
    "        '''for i in range(len(data)-n,len(data)):\n",
    "            n_days_max[i]=None\n",
    "            n_days_min[i]=None\n",
    "            '''\n",
    "    max_min_close_n_days=pd.DataFrame({'n_days_max':n_days_max,'n_days_min':n_days_min,\n",
    "                                       'n_days_max_radio':n_days_max_radio,'n_days_min_radio':n_days_min_radio})\n",
    "    max_min_close_n_days.index=data.index\n",
    "    return max_min_close_n_days\n",
    "\n",
    "def get_all_indicator(data):\n",
    "    dataset=pd.DataFrame()\n",
    "\n",
    "    indicator=QA.QA_indicator_ADTM(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ARBR(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ASI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ATR(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_BBI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    #indicator=QA.QA_indicator_BIAS(data)\n",
    "    #dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_BOLL(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_CCI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_CHO(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_DDI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_DMA(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_DMI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    #indicator=QA.QA_indicator_EMA(data)\n",
    "    #dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_EXPMA(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_KDJ(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MA(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MA_VOL(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MACD(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MFI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MIKE(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_MTM(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_OBV(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_OSC(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_PBX(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_PVT(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_ROC(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_RSI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VPT(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VR(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VRSI(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    indicator=QA.QA_indicator_VSTD(data)\n",
    "    dataset=pd.concat([dataset,indicator],axis=1)\n",
    "\n",
    "    #indicator=QA.QA_indicator_WR(data)\n",
    "    #dataset=pd.concat([dataset,indicator],axis=1)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockCode=QA.QA_fetch_stock_list()['code'].tolist()\n",
    "stockCode=stockCode[0:]\n",
    "data = QA.QA_fetch_stock_day_adv(stockCode, '2019-01-01', '2019-12-31')\n",
    "\n",
    "#1、生成标签（y）\n",
    "#获取当前股票20天后的最大涨幅和最小涨幅\n",
    "Y = data.add_func(max_min_close_n_days,20)\n",
    "#dataset=pd.concat([y,data.data], axis=1).dropna(thresh=9)\n",
    "\n",
    "#2、生成样本（x）\n",
    "dataset1=get_all_indicator(data)\n",
    "\n",
    "#3、标准化样本数据\n",
    "dataset2=standardization(data.data,(['open','close','high','low'],'volume','amount'))\n",
    "\n",
    "#####4、合并样本\n",
    "dataset=pd.concat([dataset1,dataset2],axis=1)\n",
    "\n",
    "#####5、时间序列转监督学习（n天的数据生成一个样本）\n",
    "dataset=series_to_supervised(dataset,10)\n",
    "\n",
    "#####定义Y值，加入样本中\n",
    "Y_names=['n_days_max_radio','n_days_min_radio']\n",
    "dataset=pd.concat([dataset,Y[Y_names]],axis=1)\n",
    "\n",
    "#####删除空值超过90%的样本\n",
    "len_columns=len(dataset.columns)\n",
    "dataset=dataset.dropna(thresh=len_columns*0.9)\n",
    "\n",
    "#####生成X,Y\n",
    "dataset=dataset.astype('float16')\n",
    "Y=dataset[Y_names]\n",
    "X=dataset.drop(Y_names,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l1: 0.0807654\tvalid_0's l1: 0.0806466\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[2]\ttraining's l1: 0.0801012\tvalid_0's l1: 0.0799897\n",
      "[3]\ttraining's l1: 0.0794701\tvalid_0's l1: 0.0793623\n",
      "[4]\ttraining's l1: 0.078877\tvalid_0's l1: 0.0787799\n",
      "[5]\ttraining's l1: 0.0783365\tvalid_0's l1: 0.0782508\n",
      "[6]\ttraining's l1: 0.0778281\tvalid_0's l1: 0.0777522\n",
      "[7]\ttraining's l1: 0.0773742\tvalid_0's l1: 0.0773042\n",
      "[8]\ttraining's l1: 0.0769175\tvalid_0's l1: 0.0768516\n",
      "[9]\ttraining's l1: 0.0764917\tvalid_0's l1: 0.0764359\n",
      "[10]\ttraining's l1: 0.076124\tvalid_0's l1: 0.0760759\n",
      "[11]\ttraining's l1: 0.0757648\tvalid_0's l1: 0.0757229\n",
      "[12]\ttraining's l1: 0.0754272\tvalid_0's l1: 0.0753905\n",
      "[13]\ttraining's l1: 0.0751091\tvalid_0's l1: 0.0750765\n",
      "[14]\ttraining's l1: 0.0748107\tvalid_0's l1: 0.0747834\n",
      "[15]\ttraining's l1: 0.0745153\tvalid_0's l1: 0.0744918\n",
      "[16]\ttraining's l1: 0.0742551\tvalid_0's l1: 0.0742368\n",
      "[17]\ttraining's l1: 0.0740279\tvalid_0's l1: 0.0740158\n",
      "[18]\ttraining's l1: 0.0737979\tvalid_0's l1: 0.0737925\n",
      "[19]\ttraining's l1: 0.0736065\tvalid_0's l1: 0.073605\n",
      "[20]\ttraining's l1: 0.0734136\tvalid_0's l1: 0.0734195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5371450a134d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlgb_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlgb_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlgb_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlgb_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiang/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiang/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "y=Y['n_days_max_radio']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=7)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1','mae'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train,Y_train)\n",
    "lgb_eval = [lgb.Dataset(X_test,Y_test,reference=lgb_train),lgb_train]\n",
    "model=lgb.train(params,lgb_train,num_boost_round=3000,valid_sets=lgb_eval,early_stopping_rounds=40)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353008.97959183675\n",
      "145956.15940147487\n",
      "347687.02040816325\n",
      "2.382133250381004\n"
     ]
    }
   ],
   "source": [
    "b=dataset.loc[(slice(None),'000002'),:]\n",
    "vvv=b['volume']\n",
    "print(vvv.mean())\n",
    "print(vvv.std())\n",
    "\n",
    "m=700696-vvv.mean()\n",
    "print(m)\n",
    "s=m/vvv.std()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockCode=QA.QA_fetch_stock_list()['code'].tolist()\n",
    "data = QA.QA_fetch_stock_day_adv(stockCode, '2018-03-20', '2018-08-23')\n",
    "#print(stockCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at least 2 positional arguments (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b08d5b8a6120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mAccount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQA_Account\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mBroker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQA_BacktestBroker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mQUANTAXIS\\QAARP\\QAAccount.pyx\u001b[0m in \u001b[0;36mQUANTAXIS.QAARP.QAAccount.QA_Account.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes at least 2 positional arguments (0 given)"
     ]
    }
   ],
   "source": [
    "# data.data\n",
    "# Account=QA.QA_Account()\n",
    "# Broker=QA.QA_BacktestBroker()\n",
    "user = QA.QA_user(username ='quantaxis', password = 'quantaxis')\n",
    "portfotlio=user.new_portfolio('x1')\n",
    "account = portfolio.new_account(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
